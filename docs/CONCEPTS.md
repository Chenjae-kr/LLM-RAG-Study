# RAG 핵심 개념 정리 📖

## RAG (Retrieval-Augmented Generation)란?

RAG는 **검색(Retrieval)**과 **생성(Generation)**을 결합한 AI 기법입니다.

### 기본 아이디어
```
문제: LLM이 학습하지 않은 정보는 답변할 수 없다
해결: 외부 문서를 검색해서 LLM에게 참고 자료로 제공한다
```

## RAG 동작 과정

### 1단계: 문서 준비 (Offline)
```
원본 문서
    ↓
[청킹] 작은 조각으로 분할
    ↓
[임베딩] 벡터로 변환
    ↓
[저장] 벡터 DB에 저장
```

### 2단계: 질의 응답 (Online)
```
사용자 질문
    ↓
[임베딩] 질문을 벡터로 변환
    ↓
[검색] 유사한 문서 조각 찾기
    ↓
[프롬프트 구성] 질문 + 검색된 문서
    ↓
[생성] LLM이 답변 생성
```

## 핵심 구성 요소

### 1. 임베딩 (Embedding)

**정의**: 텍스트를 숫자 벡터로 변환하는 과정

```python
"인공지능" → [0.23, -0.45, 0.67, ..., 0.12]  # 384차원 벡터
```

**특징**:
- 의미가 비슷한 텍스트 → 비슷한 벡터
- 코사인 유사도로 비교 가능

**주요 모델**:
- `all-MiniLM-L6-v2`: 빠르고 가벼움 (80MB)
- `paraphrase-multilingual`: 다국어 지원
- OpenAI `text-embedding-ada-002`: 고품질, 유료

### 2. 벡터 검색

**코사인 유사도**:
```python
similarity = cos(θ) = (A · B) / (||A|| × ||B||)
```

**결과**: -1 ~ 1 사이 값
- 1: 완전 동일
- 0: 무관
- -1: 완전 반대

### 3. 컨텍스트 윈도우

**정의**: LLM이 한 번에 처리할 수 있는 토큰 수

**예시**:
- GPT-3.5: 4,096 토큰 (~3,000 단어)
- GPT-4: 8,192 토큰 (~6,000 단어)
- Claude 3: 200,000 토큰 (~150,000 단어)

**고려사항**:
- 검색된 문서가 너무 많으면 윈도우 초과
- 너무 적으면 정보 부족
- **최적**: 1,000-2,000 토큰 정도의 컨텍스트

## 주요 기법

### 청킹 (Chunking)

긴 문서를 작은 조각으로 나누기

**방법**:
1. **고정 크기**: 512 토큰씩 분할
2. **문장 단위**: 5-10 문장씩 그룹화
3. **의미 단위**: 문단, 섹션별로 분할

**Overlap 적용**:
```
Chunk 1: [      문장1 문장2 문장3      ]
Chunk 2:          [문장2 문장3 문장4 문장5]
                   ↑
                 overlap
```

### 하이브리드 검색

**Dense (밀집) 검색**:
- 임베딩 기반 의미 검색
- 동의어, 유사 표현 찾기 좋음

**Sparse (희소) 검색**:
- 키워드 기반 (BM25)
- 정확한 용어 매칭에 강함

**결합**:
```python
final_score = α × dense_score + (1-α) × sparse_score
```

### 리랭킹 (Re-ranking)

**목적**: 초기 검색 결과를 더 정확하게 재정렬

**방법**:
1. 빠른 검색으로 후보 50개 추출
2. Cross-Encoder로 정교하게 평가
3. 상위 5개만 LLM에 전달

**효과**: 정확도 10-30% 향상

## RAG vs 다른 접근법

### RAG vs Fine-tuning

| 항목 | RAG | Fine-tuning |
|------|-----|-------------|
| 학습 | 불필요 | 필수 |
| 비용 | 낮음 | 높음 |
| 업데이트 | 즉시 | 재학습 필요 |
| 정확도 | 높음 (출처) | 높음 (내재화) |
| 사용처 | 지식 기반 | 스타일/톤 |

### RAG vs 프롬프트 엔지니어링

| 항목 | RAG | 프롬프트 |
|------|-----|----------|
| 정보량 | 대량 | 제한적 |
| 동적성 | 높음 | 낮음 |
| 복잡도 | 높음 | 낮음 |
| 비용 | 중간 | 낮음 |

## 성능 평가 지표

### Retrieval 평가

**Recall@K**: 상위 K개에 정답이 있는 비율
```python
recall = (검색된 관련 문서 수) / (전체 관련 문서 수)
```

**MRR**: 정답의 평균 역순위
```python
MRR = 평균(1 / 정답_순위)
```

### Generation 평가

**Faithfulness**: 답변이 컨텍스트에 충실한가?
**Answer Relevancy**: 답변이 질문과 관련 있는가?
**Context Precision**: 검색된 문서가 적절한가?

## 일반적인 문제와 해결

### 문제 1: 검색이 부정확함
- ✅ 더 나은 임베딩 모델 사용
- ✅ 하이브리드 검색 적용
- ✅ 리랭킹 추가

### 문제 2: 답변이 환각(Hallucination)
- ✅ 프롬프트에 "컨텍스트 기반으로만" 명시
- ✅ 낮은 temperature 사용
- ✅ 출처 포함 요구

### 문제 3: 느린 응답 속도
- ✅ 벡터 DB 인덱스 최적화
- ✅ 캐싱 적용
- ✅ 더 작은 LLM 사용

### 문제 4: 높은 비용
- ✅ 로컬 임베딩 모델 사용
- ✅ 작은 LLM (Flan-T5, Llama-7B)
- ✅ 캐싱으로 API 호출 감소

## 실전 체크리스트

### 문서 준비
- [ ] 문서가 깨끗한가? (불필요한 포맷 제거)
- [ ] 적절한 청크 크기인가? (256-512 토큰)
- [ ] Overlap이 있는가? (10-20%)
- [ ] 메타데이터가 포함되어 있는가?

### 검색
- [ ] 임베딩 모델이 언어를 지원하는가?
- [ ] Top-K 값이 적절한가? (3-10)
- [ ] 유사도 임계값을 설정했는가?
- [ ] 리랭킹을 적용했는가?

### 생성
- [ ] 프롬프트가 명확한가?
- [ ] 컨텍스트 윈도우를 초과하지 않는가?
- [ ] Temperature 값이 적절한가? (0.1-0.3)
- [ ] 출처를 포함하도록 요청했는가?

## 더 깊이 알아보기

### 추천 논문
1. **RAG** (Lewis et al., 2020) - 원본 논문
2. **DPR** (Karpukhin et al., 2020) - Dense Retrieval
3. **HyDE** (Gao et al., 2022) - 가상 문서 기법

### 추천 도구
- **LangChain**: RAG 파이프라인 구축
- **LlamaIndex**: 문서 인덱싱 특화
- **RAGAS**: 평가 프레임워크

### 추천 학습 자료
- Pinecone Learning Center
- Weaviate Blog
- DeepLearning.AI RAG Course
